# Description
**Author:** Thirith Yang, thirith.yang@students.unibe.ch

The goal of project is to provide databases for the main project (see more at [Web Scraping for a Database of Court Decision Related Documents](https://www.digitale-nachhaltigkeit.unibe.ch/studies/bachelor_s__and_master_s_theses_at_inf/natural_language_processing/web_scraping_for_a_database_of_court_decision_related_documents/index_eng.html)) about an automated system to prove true anonymity of people involved in court rulings. 

The following websites have been used to build 6 databases (.jsonl):
- legifrance: [https://www.legifrance.gouv.fr](https://www.legifrance.gouv.fr)
- gesetze_bayern: [https://www.gesetze-bayern.de](https://www.gesetze-bayern.de)
- ris_bka_gv_at: [https://www.ris.bka.gv.at](https://www.ris.bka.gv.at)
- bailii: [http://www.bailii.org](http://www.bailii.org)
- sentenze_ti: [http://www.sentenze.ti.ch](http://www.sentenze.ti.ch)
- retsinformation: [https://www.retsinformation.dk](https://www.retsinformation.dk)

For each website a python script has been created, and accordingly named. The naming convention for the Website `<name>` is `<name>.py` for the script and `<name>.jsonl` for the corresponding database.

The general assumption for the script to work is that the articles found on a website have the same structure and data format. Heterogeneity is tough to deal with, so in obvious cases I managed to ignore articles that didn't reflect the main structure, for example by using `try catch` (`try except` in python) statements as workaround.

# Requirements
The only true requirement to run the script is the proper configuration for the library used to browse the websites, namely `Selenium`.

Selenium required a webdriver to work. For this project I picked the webdriver for Chrome browser. Istruction on which version to download and how to install it can be found at this link
[Selenium - Chapter 1.5 Drivers](https://selenium-python.readthedocs.io/installation.html#drivers). Once the webdriver is properly installed the constant `PATH_CHROME` must be modified in each script to point to the executable's absolute path on your machine.

**Remark:** I choosed `Selenium` over `Requests`, an easier to use RESTful api, because of reliability reasons. Most webpages I'm scraping are dynamically generated, and after some tests and readings I came to the conclusion that it was the better pick for my project. `Selenium` allows me to proper tune the HTML fetching process to always get the fully generated webpage's HTML code.


# Script structure
Each script has roughly the same structure, only small adjustments have been made to adapt to the particularities of the scraped website.
The core parts of a scripts are listed next.

## Enviroment configuration
The sections in code *Import libraries*, *Parameters initialization* and *Webdriver initialization* are used to set up environment variables and website's specific parametes.

Examples are the variables `domain_name` and `lang`, which are respectively used to complete relative URLs and define the database's entries language.

## List of articles
The first step into gathering articles is to build a list of URLs pointing to such articles. The following functions have been used and adapted to each website needs to make the scraping possible.

---
### `pages(number)`
This function returns a webpage containing a list of articles based on the given `number`.

`number` corresponds to the current webpage displayed, just like after searching something on [www.google.com](www.google.com) you can navigate through different webpages by clicking on the number at the bottom, after the search results.

---
### Section *Build list of articles' URLs*
The loop defined in this section gathers all articles URLs found on the webpage (thanks to `pages(_)`) through the `BeautifulSoup` parser's function `find(_)` / `find_all(_)`.

## Article scraping
The following functions have been built to extract information from an article. Their bodies must be adapted to the website's HTML code in order to get the desired data.

---
### `create_article_JSON(article_soup, url, ID)`
This function builds a JSON object containing all the desired data extracted from the given article's webpage (through the `BeautifulSoup` object `article_soup`)

---
### `date_iso_format(date)`
This function is used to convert the date from a webpage to the ISO 8601 international standard format, namely `yyyy-mm-dd`.

---
### Section *Scrape articles and generate their corresponding JSON objects*
The loop defined in this section iterates through all gathered articles, and for each article it populates the list `articles_JSON` with the corresponding `JSON` object generated by the function `create_article_JSON(_)`.

## Database creation
Database are `JSONL` files. Each line in this file correspond to a `JSON` object.
The structure of a `JSON` object, containgin an article's data, is the following:

`{ ID: number, URL: string, language: string, date_decision: string, text: string }`

**Remark:** In the code there's the possibility to generate common `JSON` files as databases. As default the lines of code to generate `JSON` databases are commented. Simply comment in/out those lines at your necessity.

## Statistics
Few variables have been used to compute simple statistics on the fly. Those variable are recognized by the prefix `stat_`.